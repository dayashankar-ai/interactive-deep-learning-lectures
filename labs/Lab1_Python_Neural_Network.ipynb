print("- The network learned to separate XOR classes non-linearly!")
    

# TODO 4.1: Experiment with different architectures
# Try different hidden layer sizes and compare performance
# Test: [2, 8, 1], [2, 16, 1], and [2, 2, 1] architectures

print("\\n" + "="*60)
print("üî¨ 4.1: Architecture Comparison Experiment")
print("="*60)

architectures = [
    (2, 2, 1),   # Minimal hidden layer
    (2, 4, 1),   # Current architecture  
    (2, 8, 1),   # Larger hidden layer
    (2, 16, 1)   # Much larger hidden layer
]

results = {}

for input_size, hidden_size, output_size in architectures:
    print(f"\\nTesting architecture: {input_size}-{hidden_size}-{output_size}")
    
    # YOUR CODE HERE: Create and train networks with different architectures
    # Compare their final accuracy on XOR problem
    # Use same training parameters for fair comparison
    
    network = None  # Create your network
    losses = None   # Train your network
    
    if network is not None and losses is not None:
        # Test accuracy
        predictions = network.predict(X_xor)
        accuracy = np.mean(predictions == y_xor) * 100
        final_loss = losses[-1]
        
        results[f"{input_size}-{hidden_size}-{output_size}"] = {
            'accuracy': accuracy,
            'final_loss': final_loss,
            'epochs_to_converge': len([l for l in losses if l > 0.01])  # Rough convergence
        }
        
        print(f"Final accuracy: {accuracy:.1f}%")
        print(f"Final loss: {final_loss:.6f}")
    else:
        print("‚ùå Please implement network creation and training")

# Analysis
if results:
    print(f"\\nüìä Architecture Comparison Results:")
    print("Architecture\\tAccuracy\\tFinal Loss\\tConvergence")
    print("-" * 50)
    for arch, result in results.items():
        print(f"{arch}\\t\\t{result['accuracy']:.1f}%\\t\\t{result['final_loss']:.6f}\\t{result['epochs_to_converge']}")
    
    # Find best architecture
    best_arch = max(results.keys(), key=lambda k: results[k]['accuracy'])
    print(f"\\nüèÜ Best architecture: {best_arch}")


# TODO 4.2: Implement early stopping
# Modify the training loop to stop when loss doesn't improve for N epochs
# This prevents overfitting and saves computation time

class EarlyStoppingNetwork(NeuralNetwork):
    \"\"\"Neural network with early stopping capability\"\"\"
    
    def train_with_early_stopping(self, X, y, epochs=1000, patience=100, min_delta=1e-6, verbose=True):
        \"\"\"Train with early stopping\"\"\"
        losses = []
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # Forward and backward pass
            predictions = self.forward(X)
            loss = self.compute_loss(y, predictions)
            losses.append(loss)
            self.backward(X, y)
            
            # Check for improvement
            if loss < best_loss - min_delta:
                best_loss = loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            # Early stopping check
            if patience_counter >= patience:
                if verbose:
                    print(f"Early stopping at epoch {epoch}. Best loss: {best_loss:.6f}")
                break
            
            # Print progress
            if verbose and (epoch % 200 == 0):
                print(f"Epoch {epoch:4d}, Loss: {loss:.6f}, Patience: {patience_counter}/{patience}")
        
        return losses

# YOUR CODE HERE: Test early stopping
# Create a network and train it with early stopping
# Compare training time with regular training

print(f"\\n" + "="*50)
print("‚è∞ Testing Early Stopping")
print("="*50)

early_stop_network = None  # Create your network
early_losses = None        # Train with early stopping

if early_stop_network is not None and early_losses is not None:
    print(f"Training completed in {len(early_losses)} epochs")
    
    # Test accuracy
    final_accuracy = np.mean(early_stop_network.predict(X_xor) == y_xor) * 100
    print(f"Final accuracy: {final_accuracy:.1f}%")
    print(f"Final loss: {early_losses[-1]:.6f}")
    
    if final_accuracy >= 75:
        print("üéâ Task 4.2 completed! Early stopping works well.")
    else:
        print("‚ùå Early stopping implementation needs adjustment.")
else:
    print("‚ùå Please implement early stopping test")
    

# TODO 4.3: Create a custom dataset and solve it
# Create a more complex dataset (e.g., circular pattern) and train your network
# This tests your understanding of neural networks on real problems

print(f"\\n" + "="*60)
print("üé® Creating and Solving Custom Dataset")
print("="*60)

# YOUR CODE HERE: Create a custom 2D classification dataset
# Ideas: 
# - Circular pattern (points inside/outside a circle)
# - Spiral pattern
# - Multiple clusters
# - Checkerboard pattern

def create_circular_dataset(n_samples=100, radius=0.5, noise=0.1):
    \"\"\"Create a circular classification dataset\"\"\"
    # YOUR CODE HERE: Implement this function
    # Return X (n_samples, 2) and y (n_samples, 1)
    # Points inside circle should be class 1, outside should be class 0
    return None, None

# Test your custom dataset
X_custom, y_custom = create_circular_dataset(n_samples=200)

if X_custom is not None and y_custom is not None:
    print(f"Custom dataset created: {X_custom.shape[0]} samples")
    
    # Visualize the dataset
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    colors = ['red' if y[0] == 0 else 'blue' for y in y_custom]
    plt.scatter(X_custom[:, 0], X_custom[:, 1], c=colors, alpha=0.7)
    plt.title('Custom Dataset')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.grid(True, alpha=0.3)
    
    # Train network on custom dataset
    custom_network = NeuralNetwork(2, 8, 1, learning_rate=0.5)
    custom_losses = custom_network.train(X_custom, y_custom, epochs=1000, verbose=False)
    
    # Test accuracy
    custom_predictions = custom_network.predict(X_custom)
    custom_accuracy = np.mean(custom_predictions == y_custom) * 100
    
    print(f"Custom dataset accuracy: {custom_accuracy:.1f}%")
    
    # Visualize decision boundary
    plt.subplot(1, 2, 2)
    xx, yy = np.meshgrid(np.linspace(X_custom[:, 0].min()-0.1, X_custom[:, 0].max()+0.1, 100),
                         np.linspace(X_custom[:, 1].min()-0.1, X_custom[:, 1].max()+0.1, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = custom_network.forward(grid_points)
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, levels=50, alpha=0.7, cmap='RdYlBu')
    plt.colorbar()
    plt.scatter(X_custom[:, 0], X_custom[:, 1], c=colors, alpha=0.8, edgecolors='black')
    plt.title(f'Decision Boundary (Acc: {custom_accuracy:.1f}%)')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    
    plt.tight_layout()
    plt.show()
    
    if custom_accuracy >= 80:
        print("üéâ Task 4.3 completed! Network solved your custom problem!")
    else:
        print("ü§î Try adjusting network architecture or training parameters.")
else:
    print("‚ùå Please implement the create_circular_dataset function")
    

    ],
      "source": [
        "# TODO 4.1: Experiment with different architectures\n",
        "# Try different hidden layer sizes and compare performance\n",
        "# Test: [2, 8, 1], [2, 16, 1], and [2, 2, 1] architectures\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî¨ 4.1: Architecture Comparison Experiment\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "architectures = [\n",
        "    (2, 2, 1),   # Minimal hidden layer\n",
        "    (2, 4, 1),   # Current architecture  \n",
        "    (2, 8, 1),   # Larger hidden layer\n",
        "    (2, 16, 1)   # Much larger hidden layer\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for input_size, hidden_size, output_size in architectures:\n",
        "    print(f\"\\nTesting architecture: {input_size}-{hidden_size}-{output_size}\")\n",
        "    \n",
        "    # YOUR CODE HERE: Create and train networks with different architectures\n",
        "    # Compare their final accuracy on XOR problem\n",
        "    # Use same training parameters for fair comparison\n",
        "    \n",
        "    network = None  # Create your network\n",
        "    losses = None   # Train your network\n",
        "    \n",
        "    if network is not None and losses is not None:\n",
        "        # Test accuracy\n",
        "        predictions = network.predict(X_xor)\n",
        "        accuracy = np.mean(predictions == y_xor) * 100\n",
        "        final_loss = losses[-1]\n",
        "        \n",
        "        results[f\"{input_size}-{hidden_size}-{output_size}\"] = {\n",
        "            'accuracy': accuracy,\n",
        "            'final_loss': final_loss,\n",
        "            'epochs_to_converge': len([l for l in losses if l > 0.01])  # Rough convergence\n",
        "        }\n",
        "        \n",
        "        print(f\"Final accuracy: {accuracy:.1f}%\")\n",
        "        print(f\"Final loss: {final_loss:.6f}\")\n",
        "    else:\n",
        "        print(\"‚ùå Please implement network creation and training\")\n",
        "\n",
        "# Analysis\n",
        "if results:\n",
        "    print(f\"\\nüìä Architecture Comparison Results:\")\n",
        "    print(\"Architecture\\tAccuracy\\tFinal Loss\\tConvergence\")\n",
        "    print(\"-\" * 50)\n",
        "    for arch, result in results.items():\n",
        "        print(f\"{arch}\\t\\t{result['accuracy']:.1f}%\\t\\t{result['final_loss']:.6f}\\t{result['epochs_to_converge']}\")\n",
        "    \n",
        "    # Find best architecture\n",
        "    best_arch = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
        "    print(f\"\\nüèÜ Best architecture: {best_arch}\")\n",
        "\n",
        "\n",
        "# TODO 4.2: Implement early stopping\n",
        "# Modify the training loop to stop when loss doesn't improve for N epochs\n",
        "# This prevents overfitting and saves computation time\n",
        "\n",
        "class EarlyStoppingNetwork(NeuralNetwork):\n",
        "    \"\"\"Neural network with early stopping capability\"\"\"\n",
        "    \n",
        "    def train_with_early_stopping(self, X, y, epochs=1000, patience=100, min_delta=1e-6, verbose=True):\n",
        "        \"\"\"Train with early stopping\"\"\"\n",
        "        losses = []\n",
        "        best_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward and backward pass\n",
        "            predictions = self.forward(X)\n",
        "            loss = self.compute_loss(y, predictions)\n",
        "            losses.append(loss)\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Check for improvement\n",
        "            if loss < best_loss - min_delta:\n",
        "                best_loss = loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # Early stopping check\n",
        "            if patience_counter >= patience:\n",
        "                if verbose:\n",
        "                    print(f\"Early stopping at epoch {epoch}. Best loss: {best_loss:.6f}\")\n",
        "                break\n",
        "            \n",
        "            # Print progress\n",
        "            if verbose and (epoch % 200 == 0):\n",
        "                print(f\"Epoch {epoch:4d}, Loss: {loss:.6f}, Patience: {patience_counter}/{patience}\")\n",
        "        \n",
        "        return losses\n",
        "\n",
        "# YOUR CODE HERE: Test early stopping\n",
        "# Create a network and train it with early stopping\n",
        "# Compare training time with regular training\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"‚è∞ Testing Early Stopping\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "early_stop_network = None  # Create your network\n",
        "early_losses = None        # Train with early stopping\n",
        "\n",
        "if early_stop_network is not None and early_losses is not None:\n",
        "    print(f\"Training completed in {len(early_losses)} epochs\")\n",
        "    \n",
        "    # Test accuracy\n",
        "    final_accuracy = np.mean(early_stop_network.predict(X_xor) == y_xor) * 100\n",
        "    print(f\"Final accuracy: {final_accuracy:.1f}%\")\n",
        "    print(f\"Final loss: {early_losses[-1]:.6f}\")\n",
        "    \n",
        "    if final_accuracy >= 75:\n",
        "        print(\"üéâ Task 4.2 completed! Early stopping works well.\")\n",
        "    else:\n",
        "        print(\"‚ùå Early stopping implementation needs adjustment.\")\n",
        "else:\n",
        "    print(\"‚ùå Please implement early stopping test\")\n",
        "    \n",
        "\n",
        "# TODO 4.3: Create a custom dataset and solve it\n",
        "# Create a more complex dataset (e.g., circular pattern) and train your network\n",
        "# This tests your understanding of neural networks on real problems\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"üé® Creating and Solving Custom Dataset\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# YOUR CODE HERE: Create a custom 2D classification dataset\n",
        "# Ideas: \n",
        "# - Circular pattern (points inside/outside a circle)\n",
        "# - Spiral pattern\n",
        "# - Multiple clusters\n",
        "# - Checkerboard pattern\n",
        "\n",
        "def create_circular_dataset(n_samples=100, radius=0.5, noise=0.1):\n",
        "    \"\"\"Create a circular classification dataset\"\"\"\n",
        "    # YOUR CODE HERE: Implement this function\n",
        "    # Return X (n_samples, 2) and y (n_samples, 1)\n",
        "    # Points inside circle should be class 1, outside should be class 0\n",
        "    return None, None\n",
        "\n",
        "# Test your custom dataset\n",
        "X_custom, y_custom = create_circular_dataset(n_samples=200)\n",
        "\n",
        "if X_custom is not None and y_custom is not None:\n",
        "    print(f\"Custom dataset created: {X_custom.shape[0]} samples\")\n",
        "    \n",
        "    # Visualize the dataset\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    colors = ['red' if y[0] == 0 else 'blue' for y in y_custom]\n",
        "    plt.scatter(X_custom[:, 0], X_custom[:, 1], c=colors, alpha=0.7)\n",
        "    plt.title('Custom Dataset')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Train network on custom dataset\n",
        "    custom_network = NeuralNetwork(2, 8, 1, learning_rate=0.5)\n",
        "    custom_losses = custom_network.train(X_custom, y_custom, epochs=1000, verbose=False)\n",
        "    \n",
        "    # Test accuracy\n",
        "    custom_predictions = custom_network.predict(X_custom)\n",
        "    custom_accuracy = np.mean(custom_predictions == y_custom) * 100\n",
        "    \n",
        "    print(f\"Custom dataset accuracy: {custom_accuracy:.1f}%\")\n",
        "    \n",
        "    # Visualize decision boundary\n",
        "    plt.subplot(1, 2, 2)\n",
        "    xx, yy = np.meshgrid(np.linspace(X_custom[:, 0].min()-0.1, X_custom[:, 0].max()+0.1, 100),\n",
        "                         np.linspace(X_custom[:, 1].min()-0.1, X_custom[:, 1].max()+0.1, 100))\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = custom_network.forward(grid_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.7, cmap='RdYlBu')\n",
        "    plt.colorbar()\n",
        "    plt.scatter(X_custom[:, 0], X_custom[:, 1], c=colors, alpha=0.8, edgecolors='black')\n",
        "    plt.title(f'Decision Boundary (Acc: {custom_accuracy:.1f}%)')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    if custom_accuracy >= 80:\n",
        "        print(\"üéâ Task 4.3 completed! Network solved your custom problem!\")\n",
        "    else:\n",
        "        print(\"ü§î Try adjusting network architecture or training parameters.\")\n",
        "else:\n",
        "    print(\"‚ùå Please implement the create_circular_dataset function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise5_header"
      },
      "source": [
        "---\n",
        "# üìä Exercise 5: Visualization & Analysis (20 minutes)\n",
        "\n",
        "Create comprehensive visualizations to understand how your neural network learns. This is crucial for debugging and improving your models.\n",
        "\n",
        "## üéØ Learning Goals:\n",
        "- Create informative training visualizations\n",
        "- Analyze network behavior and performance\n",
        "- Understand weight evolution during training\n",
        "- Generate comprehensive reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise5_comprehensive_analysis"
      },
      "outputs": [],
      "source": [
        "# 5.1 Comprehensive Network Analysis\n",
        "print(\"üìä 5.1: Comprehensive Network Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def analyze_network_performance(network, X, y, losses, title=\"Network Analysis\"):\n",
        "    \"\"\"Create comprehensive analysis of network performance\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Training Loss\n",
        "    axes[0, 0].plot(losses, 'b-', linewidth=2)\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].set_yscale('log')\n",
        "    \n",
        "    # 2. Loss Improvement Rate\n",
        "    loss_diff = np.diff(losses)\n",
        "    axes[0, 1].plot(loss_diff, 'r-', alpha=0.7)\n",
        "    axes[0, 1].set_title('Loss Change Rate')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss Difference')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    # 3. Predictions vs Targets\n",
        "    predictions = network.forward(X)\n",
        "    axes[0, 2].scatter(y.flatten(), predictions.flatten(), alpha=0.7)\n",
        "    axes[0, 2].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', alpha=0.8)\n",
        "    axes[0, 2].set_title('Predictions vs Targets')\n",
        "    axes[0, 2].set_xlabel('True Values')\n",
        "    axes[0, 2].set_ylabel('Predictions')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Residuals (Prediction Errors)\n",
        "    residuals = y.flatten() - predictions.flatten()\n",
        "    axes[1, 0].scatter(predictions.flatten(), residuals, alpha=0.7)\n",
        "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
        "    axes[1, 0].set_title('Residual Plot')\n",
        "    axes[1, 0].set_xlabel('Predictions')\n",
        "    axes[1, 0].set_ylabel('Residuals')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Weight Distributions\n",
        "    all_weights = np.concatenate([network.W1.flatten(), network.W2.flatten()])\n",
        "    axes[1, 1].hist(all_weights, bins=20, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_title('Weight Distribution')\n",
        "    axes[1, 1].set_xlabel('Weight Value')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Network Architecture Visualization\n",
        "    axes[1, 2].text(0.1, 0.8, f'Architecture: {network.input_size}-{network.hidden_size}-{network.output_size}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    axes[1, 2].text(0.1, 0.7, f'Parameters: {network.count_parameters()}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    axes[1, 2].text(0.1, 0.6, f'Final Loss: {losses[-1]:.6f}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    axes[1, 2].text(0.1, 0.5, f'Training Epochs: {len(losses)}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    \n",
        "    # Calculate and display metrics\n",
        "    mse = np.mean(residuals**2)\n",
        "    mae = np.mean(np.abs(residuals))\n",
        "    \n",
        "    axes[1, 2].text(0.1, 0.3, f'MSE: {mse:.6f}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    axes[1, 2].text(0.1, 0.2, f'MAE: {mae:.6f}', \n",
        "                     fontsize=12, transform=axes[1, 2].transAxes)\n",
        "    \n",
        "    axes[1, 2].set_title('Network Summary')\n",
        "    axes[1, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'final_loss': losses[-1],\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'training_epochs': len(losses)\n",
        "    }\n",
        "\n",
        "# Analyze our XOR network\n",
        "print(\"\\nüîç Analyzing XOR Network Performance:\")\n",
        "xor_analysis = analyze_network_performance(xor_network, X_xor, y_xor, training_losses, \n",
        "                                          \"XOR Network Analysis\")\n",
        "\n",
        "print(f\"\\nüìà XOR Network Metrics:\")\n",
        "for metric, value in xor_analysis.items():\n",
        "    print(f\"  {metric}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise5_weight_evolution"
      },
      "outputs": [],
      "source": [
        "# 5.2 Weight Evolution During Training\n",
        "print(\"\\n‚öñÔ∏è 5.2: Tracking Weight Evolution\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class TrackingNetwork(NeuralNetwork):\n",
        "    \"\"\"Neural network that tracks weight evolution during training\"\"\"\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weight_history = {\n",
        "            'W1': [],\n",
        "            'b1': [],\n",
        "            'W2': [],\n",
        "            'b2': []\n",
        "        }\n",
        "    \n",
        "    def track_weights(self):\n",
        "        \"\"\"Store current weights\"\"\"\n",
        "        self.weight_history['W1'].append(self.W1.copy())\n",
        "        self.weight_history['b1'].append(self.b1.copy())\n",
        "        self.weight_history['W2'].append(self.W2.copy())\n",
        "        self.weight_history['b2'].append(self.b2.copy())\n",
        "    \n",
        "    def train_with_tracking(self, X, y, epochs=1000, track_every=50):\n",
        "        \"\"\"Train network while tracking weight evolution\"\"\"\n",
        "        losses = []\n",
        "        \n",
        "        # Track initial weights\n",
        "        self.track_weights()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward and backward pass\n",
        "            predictions = self.forward(X)\n",
        "            loss = self.compute_loss(y, predictions)\n",
        "            losses.append(loss)\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Track weights periodically\n",
        "            if epoch % track_every == 0:\n",
        "                self.track_weights()\n",
        "            \n",
        "            if epoch % 200 == 0:\n",
        "                print(f\"Epoch {epoch:4d}, Loss: {loss:.6f}\")\n",
        "        \n",
        "        # Track final weights\n",
        "        self.track_weights()\n",
        "        \n",
        "        return losses\n",
        "    \n",
        "    def plot_weight_evolution(self):\n",
        "        \"\"\"Visualize how weights change during training\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Weight Evolution During Training', fontsize=16)\n",
        "        \n",
        "        # Plot W1 evolution\n",
        "        W1_evolution = np.array(self.weight_history['W1'])\n",
        "        for i in range(W1_evolution.shape[1]):\n",
        "            for j in range(W1_evolution.shape[2]):\n",
        "                axes[0, 0].plot(W1_evolution[:, i, j], alpha=0.7, \n",
        "                               label=f'W1[{i},{j}]' if W1_evolution.shape[1] <= 3 else None)\n",
        "        axes[0, 0].set_title('Hidden Layer Weights (W1)')\n",
        "        axes[0, 0].set_xlabel('Tracking Point')\n",
        "        axes[0, 0].set_ylabel('Weight Value')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        if W1_evolution.shape[1] <= 3:\n",
        "            axes[0, 0].legend()\n",
        "        \n",
        "        # Plot b1 evolution\n",
        "        b1_evolution = np.array(self.weight_history['b1']).squeeze()\n",
        "        for i in range(b1_evolution.shape[1]):\n",
        "            axes[0, 1].plot(b1_evolution[:, i], alpha=0.7, label=f'b1[{i}]')\n",
        "        axes[0, 1].set_title('Hidden Layer Biases (b1)')\n",
        "        axes[0, 1].set_xlabel('Tracking Point')\n",
        "        axes[0, 1].set_ylabel('Bias Value')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].legend()\n",
        "        \n",
        "        # Plot W2 evolution\n",
        "        W2_evolution = np.array(self.weight_history['W2'])\n",
        "        for i in range(W2_evolution.shape[1]):\n",
        "            for j in range(W2_evolution.shape[2]):\n",
        "                axes[1, 0].plot(W2_evolution[:, i, j], alpha=0.7, label=f'W2[{i},{j}]')\n",
        "        axes[1, 0].set_title('Output Layer Weights (W2)')\n",
        "        axes[1, 0].set_xlabel('Tracking Point')\n",
        "        axes[1, 0].set_ylabel('Weight Value')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].legend()\n",
        "        \n",
        "        # Plot b2 evolution\n",
        "        b2_evolution = np.array(self.weight_history['b2']).squeeze()\n",
        "        axes[1, 1].plot(b2_evolution, 'r-', linewidth=2, label='b2[0]')\n",
        "        axes[1, 1].set_title('Output Layer Bias (b2)')\n",
        "        axes[1, 1].set_xlabel('Tracking Point')\n",
        "        axes[1, 1].set_ylabel('Bias Value')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Train a network with weight tracking\n",
        "print(\"Training network with weight tracking...\")\n",
        "tracking_network = TrackingNetwork(2, 4, 1, learning_rate=1.0)\n",
        "tracking_losses = tracking_network.train_with_tracking(X_xor, y_xor, epochs=1000, track_every=25)\n",
        "\n",
        "print(\"\\nüìä Visualizing weight evolution:\")\n",
        "tracking_network.plot_weight_evolution()\n",
        "\n",
        "print(\"\\nüí° Weight Evolution Insights:\")\n",
        "print(\"- Weights start random and gradually converge to optimal values\")\n",
        "print(\"- Some weights may oscillate before settling\")\n",
        "print(\"- Biases often show smoother changes than weights\")\n",
        "print(\"- Final weight values encode the learned XOR function!\")"
      ]else:
    print(f"\\n‚ùå Leaky ReLU output incorrect. Expected -0.01, got {output_neg:.6f}")
    

# TODO 3.3: Neuron learning simulation
# Simulate how a neuron learns to solve a simple problem
# Problem: Learn to compute OR gate (True if any input is True)
# Training data: [0,0]->0, [0,1]->1, [1,0]->1, [1,1]->1

# Training data for OR gate
or_training_data = [
    ([0, 0], 0),
    ([0, 1], 1),
    ([1, 0], 1),
    ([1, 1], 1)
]

# Create and train a neuron
learning_neuron = Neuron(2, activation='sigmoid')
print(f"\\nüéì Training Neuron to Learn OR Gate:")
print(f"Initial weights: {learning_neuron.weights}")
print(f"Initial bias: {learning_neuron.bias:.4f}")

learning_rate = 1.0
epochs = 100

print(f"\nTraining for {epochs} epochs with learning rate {learning_rate}...")
print("Epoch\tLoss\tPredictions")
print("-" * 40)

for epoch in range(epochs):
    total_loss = 0
    
    # YOUR CODE HERE: Implement training loop
    # For each training example:
    # 1. Forward pass: get prediction
    # 2. Calculate error (target - prediction)
    # 3. Update weights using simple rule: weight += learning_rate * error * input
    # 4. Update bias using: bias += learning_rate * error
    
    # Hint: Use the sigmoid derivative for more accurate updates
    # sigmoid_derivative = output * (1 - output)
    
    for inputs, target in or_training_data:
        # Forward pass
        prediction, weighted_sum = learning_neuron.forward(inputs)
        
        # Calculate error and loss
        error = target - prediction
        loss = 0.5 * error ** 2
        total_loss += loss
        
        # Sigmoid derivative for gradient calculation
        sigmoid_deriv = prediction * (1 - prediction)
        
        # Calculate gradients
        gradient = error * sigmoid_deriv
        
        # Update weights and bias (implement this!)
        # learning_neuron.weights += ...
        # learning_neuron.bias += ...
        
    # Print progress every 20 epochs
    if epoch % 20 == 0 or epoch == epochs - 1:
        predictions = []
        for inputs, target in or_training_data:
            pred, _ = learning_neuron.forward(inputs)
            predictions.append(f"{pred:.3f}")
        pred_str = " ".join(predictions)
        print(f"{epoch:3d}\t{total_loss:.4f}\t{pred_str}")

print(f"\nFinal weights: {learning_neuron.weights}")
print(f"Final bias: {learning_neuron.bias:.4f}")

# Test final performance
print(f"\nüß™ Final Test Results:")
print("Input\tTarget\tPrediction\tCorrect?")
print("-" * 35)
all_correct = True
for inputs, target in or_training_data:
    prediction, _ = learning_neuron.forward(inputs)
    predicted_class = 1 if prediction > 0.5 else 0
    correct = "‚úÖ" if predicted_class == target else "‚ùå"
    if predicted_class != target:
        all_correct = False
    print(f"{inputs}\t{target}\t{prediction:.3f}\t\t{correct}")

if all_correct:
    print("\nüéâ Task 3.3 completed! Neuron learned the OR gate!")
else:
    print("\n‚ùå Neuron didn't learn correctly. Check your training implementation."){
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lab1_header"
      },
      "source": [
        "# üêç Lab 1: Python Foundations & First Neural Network\n",
        "\n",
        "**Course:** Deep Learning Mastery | **Instructor:** Dr. Daya Shankar | **Duration:** 2 Hours\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this lab, you will:\n",
        "- Master essential Python libraries for deep learning\n",
        "- Implement mathematical operations using NumPy\n",
        "- Build your first neural network from scratch\n",
        "- Understand forward and backpropagation\n",
        "- Visualize neural network learning\n",
        "\n",
        "## ‚ö†Ô∏è Setup Instructions\n",
        "1. **Save a copy**: File ‚Üí Save a copy in Drive\n",
        "2. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "3. **Run all setup cells** before starting exercises\n",
        "4. **Share with instructor**: dayashankar.ai@gmail.com\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "# üîß Setup & Environment Check\n",
        "\n",
        "Let's start by setting up our environment and importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_imports"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import optimize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üéâ All libraries imported successfully!\")\n",
        "print(f\"üìä NumPy version: {np.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üêº Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available (optional but recommended)\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def check_gpu():\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"üöÄ GPU is available!\")\n",
        "            print(\"GPU Details:\")\n",
        "            lines = result.stdout.split('\\n')\n",
        "            for line in lines[8:11]:  # GPU info lines\n",
        "                if 'Tesla' in line or 'GeForce' in line or 'Quadro' in line:\n",
        "                    print(f\"   {line.strip()}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è GPU not detected. Using CPU (slower but still works!)\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è GPU not detected. Using CPU (slower but still works!)\")\n",
        "\n",
        "check_gpu()\n",
        "\n",
        "# Create a test array to verify NumPy is working\n",
        "test_array = np.array([1, 2, 3, 4, 5])\n",
        "print(f\"\\n‚úÖ NumPy test successful: {test_array}\")\n",
        "print(f\"‚úÖ Array operations work: sum = {np.sum(test_array)}, mean = {np.mean(test_array)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1_header"
      },
      "source": [
        "---\n",
        "# üìä Exercise 1: Python & NumPy Fundamentals (20 minutes)\n",
        "\n",
        "Master the essential Python tools for deep learning. We'll work with arrays, matrices, and basic operations that form the foundation of neural networks.\n",
        "\n",
        "## üéØ Learning Goals:\n",
        "- Create and manipulate NumPy arrays\n",
        "- Perform matrix operations\n",
        "- Understand broadcasting\n",
        "- Master array indexing and slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1_arrays"
      },
      "outputs": [],
      "source": [
        "# 1.1 Array Creation and Basic Operations\n",
        "print(\"üî¢ 1.1: Array Creation and Basic Operations\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create different types of arrays\n",
        "arr1 = np.array([1, 2, 3, 4, 5])\n",
        "arr2 = np.zeros((3, 4))\n",
        "arr3 = np.ones((2, 3))\n",
        "arr4 = np.random.randn(2, 3)\n",
        "arr5 = np.arange(0, 10, 2)\n",
        "arr6 = np.linspace(0, 1, 5)\n",
        "\n",
        "print(f\"Basic array: {arr1}\")\n",
        "print(f\"Zeros array shape {arr2.shape}:\\n{arr2}\")\n",
        "print(f\"Ones array shape {arr3.shape}:\\n{arr3}\")\n",
        "print(f\"Random array shape {arr4.shape}:\\n{arr4}\")\n",
        "print(f\"Arange array: {arr5}\")\n",
        "print(f\"Linspace array: {arr6}\")\n",
        "\n",
        "# Array properties\n",
        "print(f\"\\nüìê Array Properties:\")\n",
        "print(f\"arr1 shape: {arr1.shape}, dtype: {arr1.dtype}, size: {arr1.size}\")\n",
        "print(f\"arr4 shape: {arr4.shape}, dtype: {arr4.dtype}, size: {arr4.size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1_matrix_ops"
      },
      "outputs": [],
      "source": [
        "# 1.2 Matrix Operations (Core of Neural Networks!)\n",
        "print(\"üßÆ 1.2: Matrix Operations\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create matrices representing neural network components\n",
        "# Think: inputs = data samples, weights = learned parameters\n",
        "inputs = np.array([[1, 2, 3],    # Sample 1: features [x1, x2, x3]\n",
        "                   [4, 5, 6],    # Sample 2: features [x1, x2, x3]\n",
        "                   [7, 8, 9]])   # Sample 3: features [x1, x2, x3]\n",
        "\n",
        "weights = np.array([[0.1, 0.2],  # Weights from 3 inputs to 2 outputs\n",
        "                    [0.3, 0.4],\n",
        "                    [0.5, 0.6]])\n",
        "\n",
        "print(f\"Input matrix (3 samples, 3 features):\\n{inputs}\")\n",
        "print(f\"\\nWeight matrix (3 features to 2 neurons):\\n{weights}\")\n",
        "\n",
        "# Matrix multiplication - core operation in neural networks!\n",
        "output = np.dot(inputs, weights)\n",
        "print(f\"\\nOutput (3 samples, 2 neurons):\\n{output}\")\n",
        "\n",
        "# Alternative ways to multiply matrices\n",
        "output_alt = inputs @ weights  # Python 3.5+ operator\n",
        "print(f\"\\n‚úÖ Alternative @ operator gives same result: {np.allclose(output, output_alt)}\")\n",
        "\n",
        "# Element-wise operations\n",
        "squared = inputs ** 2\n",
        "print(f\"\\nElement-wise squaring:\\n{squared}\")\n",
        "\n",
        "# Broadcasting example\n",
        "bias = np.array([0.1, 0.2])  # Bias for each neuron\n",
        "output_with_bias = output + bias  # Broadcasting!\n",
        "print(f\"\\nOutput with bias (broadcasting):\\n{output_with_bias}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1_indexing"
      },
      "outputs": [],
      "source": [
        "# 1.3 Advanced Indexing and Slicing\n",
        "print(\"üéØ 1.3: Advanced Indexing and Slicing\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create a sample dataset\n",
        "data = np.random.randn(5, 4)  # 5 samples, 4 features\n",
        "print(f\"Sample dataset (5 samples, 4 features):\\n{data}\")\n",
        "\n",
        "# Basic indexing\n",
        "print(f\"\\nFirst sample: {data[0]}\")\n",
        "print(f\"Last sample: {data[-1]}\")\n",
        "print(f\"First feature of all samples: {data[:, 0]}\")\n",
        "print(f\"First 3 samples, last 2 features:\\n{data[:3, -2:]}\")\n",
        "\n",
        "# Boolean indexing (very useful for data filtering!)\n",
        "positive_mask = data > 0\n",
        "positive_values = data[positive_mask]\n",
        "print(f\"\\nNumber of positive values: {len(positive_values)}\")\n",
        "print(f\"First 5 positive values: {positive_values[:5]}\")\n",
        "\n",
        "# Fancy indexing\n",
        "indices = [0, 2, 4]  # Select samples 0, 2, and 4\n",
        "selected_samples = data[indices]\n",
        "print(f\"\\nSelected samples (0, 2, 4):\\n{selected_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1_todo"
      },
      "source": [
        "## üöÄ **YOUR TURN: Exercise 1 Tasks**\n",
        "\n",
        "Complete the following tasks to solidify your understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1_task1"
      },
      "outputs": [],
      "source": [
        "# TODO 1.1: Create a neural network weight matrix\n",
        "# Create a random weight matrix for a neural network with:\n",
        "# - 4 input features\n",
        "# - 6 hidden neurons\n",
        "# Use normal distribution with mean=0, std=0.1\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "weight_matrix = None  # Replace with your code\n",
        "\n",
        "# Test your solution\n",
        "if weight_matrix is not None:\n",
        "    print(f\"‚úÖ Weight matrix shape: {weight_matrix.shape}\")\n",
        "    print(f\"‚úÖ Weight matrix mean: {np.mean(weight_matrix):.4f}\")\n",
        "    print(f\"‚úÖ Weight matrix std: {np.std(weight_matrix):.4f}\")\n",
        "    if weight_matrix.shape == (4, 6) and abs(np.std(weight_matrix) - 0.1) < 0.05:\n",
        "        print(\"üéâ Task 1.1 completed successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Check your matrix dimensions and standard deviation\")\n",
        "else:\n",
        "    print(\"‚ùå Please implement weight_matrix creation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1_task2"
      },
      "outputs": [],
      "source": [
        "# TODO 1.2: Matrix multiplication for mini-batch processing\n",
        "# Create a mini-batch of 3 samples with 4 features each\n",
        "# Multiply with the weight matrix from Task 1.1\n",
        "# Add a bias vector of shape (6,) with values [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "mini_batch = None      # Replace with your code\n",
        "bias_vector = None     # Replace with your code\n",
        "layer_output = None    # Replace with your code\n",
        "\n",
        "# Test your solution\n",
        "if all(v is not None for v in [mini_batch, bias_vector, layer_output]):\n",
        "    print(f\"‚úÖ Mini-batch shape: {mini_batch.shape}\")\n",
        "    print(f\"‚úÖ Bias vector shape: {bias_vector.shape}\")\n",
        "    print(f\"‚úÖ Layer output shape: {layer_output.shape}\")\n",
        "    \n",
        "    if (mini_batch.shape == (3, 4) and \n",
        "        bias_vector.shape == (6,) and \n",
        "        layer_output.shape == (3, 6)):\n",
        "        print(\"üéâ Task 1.2 completed successfully!\")\n",
        "        print(f\"Sample output (first sample): {layer_output[0]}\")\n",
        "    else:\n",
        "        print(\"‚ùå Check your array shapes\")\n",
        "else:\n",
        "    print(\"‚ùå Please implement all variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise2_header"
      },
      "source": [
        "---\n",
        "# üßÆ Exercise 2: Mathematical Operations in Code (20 minutes)\n",
        "\n",
        "Implement the mathematical concepts from Lecture 2 using Python. You'll code derivatives, gradients, and see how calculus comes alive in programming.\n",
        "\n",
        "## üéØ Learning Goals:\n",
        "- Compute derivatives numerically\n",
        "- Implement gradient descent\n",
        "- Visualize mathematical functions\n",
        "- Connect math to neural network training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_derivatives"
      },
      "outputs": [],
      "source": [
        "# 2.1 Numerical Derivatives\n",
        "print(\"üìà 2.1: Computing Derivatives Numerically\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def compute_derivative(func, x, h=1e-5):\n",
        "    \"\"\"Compute derivative using finite differences (central difference)\"\"\"\n",
        "    return (func(x + h) - func(x - h)) / (2 * h)\n",
        "\n",
        "# Define test functions\n",
        "def quadratic(x):\n",
        "    \"\"\"f(x) = x¬≤ + 2x + 1\"\"\"\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "def quadratic_derivative(x):\n",
        "    \"\"\"Analytical derivative: f'(x) = 2x + 2\"\"\"\n",
        "    return 2*x + 2\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Analytical derivative of sigmoid\"\"\"\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Test derivative computation\n",
        "test_points = [-2, -1, 0, 1, 2]\n",
        "\n",
        "print(\"Quadratic function f(x) = x¬≤ + 2x + 1:\")\n",
        "print(\"x\\tNumerical\\tAnalytical\\tError\")\n",
        "print(\"-\" * 40)\n",
        "for x in test_points:\n",
        "    numerical = compute_derivative(quadratic, x)\n",
        "    analytical = quadratic_derivative(x)\n",
        "    error = abs(numerical - analytical)\n",
        "    print(f\"{x}\\t{numerical:.6f}\\t{analytical:.6f}\\t{error:.8f}\")\n",
        "\n",
        "print(\"\\nSigmoid function:\")\n",
        "print(\"x\\tNumerical\\tAnalytical\\tError\")\n",
        "print(\"-\" * 40)\n",
        "for x in test_points:\n",
        "    numerical = compute_derivative(sigmoid, x)\n",
        "    analytical = sigmoid_derivative(x)\n",
        "    error = abs(numerical - analytical)\n",
        "    print(f\"{x}\\t{numerical:.6f}\\t{analytical:.6f}\\t{error:.8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_visualization"
      },
      "outputs": [],
      "source": [
        "# 2.2 Function Visualization\n",
        "print(\"üìä 2.2: Visualizing Functions and Their Derivatives\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create x values for plotting\n",
        "x = np.linspace(-3, 3, 1000)\n",
        "\n",
        "# Plot quadratic function and its derivative\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Quadratic function\n",
        "axes[0].plot(x, quadratic(x), 'b-', linewidth=2, label='f(x) = x¬≤ + 2x + 1')\n",
        "axes[0].plot(x, quadratic_derivative(x), 'r--', linewidth=2, label=\"f'(x) = 2x + 2\")\n",
        "axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "axes[0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Quadratic Function and Derivative')\n",
        "axes[0].set_xlabel('x')\n",
        "axes[0].set_ylabel('y')\n",
        "\n",
        "# Sigmoid function\n",
        "axes[1].plot(x, sigmoid(x), 'b-', linewidth=2, label='œÉ(x) = 1/(1+e‚ÅªÀ£)')\n",
        "axes[1].plot(x, sigmoid_derivative(x), 'r--', linewidth=2, label=\"œÉ'(x) = œÉ(x)(1-œÉ(x))\")\n",
        "axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "axes[1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Sigmoid Activation Function and Derivative')\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_ylabel('y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Observations:\")\n",
        "print(\"- Derivative shows the slope of the function at each point\")\n",
        "print(\"- Where derivative = 0, the function has a minimum/maximum\")\n",
        "print(\"- Sigmoid derivative is maximum at x=0 (steepest part of sigmoid)\")\n",
        "print(\"- These derivatives are crucial for neural network training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_gradient_descent"
      },
      "outputs": [],
      "source": [
        "# 2.3 Gradient Descent Implementation\n",
        "print(\"üéØ 2.3: Gradient Descent - How Neural Networks Learn\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def gradient_descent_1d(func, derivative_func, start_x, learning_rate=0.1, iterations=50):\n",
        "    \"\"\"Perform gradient descent on a 1D function\"\"\"\n",
        "    x = start_x\n",
        "    history = {'x': [x], 'y': [func(x)]}\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        # Compute gradient (derivative)\n",
        "        gradient = derivative_func(x)\n",
        "        \n",
        "        # Update x in opposite direction of gradient\n",
        "        x = x - learning_rate * gradient\n",
        "        \n",
        "        # Store history\n",
        "        history['x'].append(x)\n",
        "        history['y'].append(func(x))\n",
        "        \n",
        "        # Print progress every 10 iterations\n",
        "        if i % 10 == 0 or i == iterations - 1:\n",
        "            print(f\"Iteration {i:2d}: x = {x:8.5f}, f(x) = {func(x):8.5f}, gradient = {gradient:8.5f}\")\n",
        "    \n",
        "    return x, history\n",
        "\n",
        "# Test on quadratic function (has global minimum at x = -1)\n",
        "print(\"Finding minimum of f(x) = x¬≤ + 2x + 1:\")\n",
        "print(\"(Analytical minimum is at x = -1)\")\n",
        "print()\n",
        "\n",
        "final_x, history = gradient_descent_1d(quadratic, quadratic_derivative, \n",
        "                                       start_x=3.0, learning_rate=0.1, iterations=30)\n",
        "\n",
        "print(f\"\\nüéØ Final result: x = {final_x:.6f}, f(x) = {quadratic(final_x):.6f}\")\n",
        "print(f\"üéØ Analytical minimum: x = -1.000000, f(x) = {quadratic(-1.0):.6f}\")\n",
        "print(f\"‚úÖ Error: {abs(final_x - (-1.0)):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_gradient_viz"
      },
      "outputs": [],
      "source": [
        "# 2.4 Visualize Gradient Descent\n",
        "print(\"üìà 2.4: Visualizing Gradient Descent Process\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create detailed plot\n",
        "x_plot = np.linspace(-4, 4, 1000)\n",
        "y_plot = quadratic(x_plot)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot function\n",
        "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x¬≤ + 2x + 1', alpha=0.7)\n",
        "\n",
        "# Plot gradient descent path\n",
        "x_history = np.array(history['x'])\n",
        "y_history = np.array(history['y'])\n",
        "\n",
        "# Plot the path\n",
        "plt.plot(x_history, y_history, 'ro-', linewidth=2, markersize=6, \n",
        "         label='Gradient Descent Path', alpha=0.8)\n",
        "\n",
        "# Mark start and end points\n",
        "plt.plot(x_history[0], y_history[0], 'go', markersize=10, label='Start')\n",
        "plt.plot(x_history[-1], y_history[-1], 'rs', markersize=10, label='End')\n",
        "\n",
        "# Mark true minimum\n",
        "plt.plot(-1, quadratic(-1), 'k*', markersize=15, label='True Minimum')\n",
        "\n",
        "# Add arrows to show direction\n",
        "for i in range(0, len(x_history)-1, 3):  # Every 3rd point\n",
        "    dx = x_history[i+1] - x_history[i]\n",
        "    dy = y_history[i+1] - y_history[i]\n",
        "    plt.arrow(x_history[i], y_history[i], dx*0.8, dy*0.8, \n",
        "              head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.6)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title('Gradient Descent Optimization\\n(Watch the red dot roll downhill!)')\n",
        "plt.legend()\n",
        "plt.xlim(-4, 4)\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Insights:\")\n",
        "print(\"- Gradient descent follows the steepest descent direction\")\n",
        "print(\"- Learning rate controls step size (too big = overshoot, too small = slow)\")\n",
        "print(\"- This is exactly how neural networks update their weights!\")\n",
        "print(f\"- Converged in {len(history['x'])-1} iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise2_todo"
      },
      "source": [
        "## üöÄ **YOUR TURN: Exercise 2 Tasks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_task1"
      },
      "outputs": [],
      "source": [
        "# TODO 2.1: Implement ReLU activation and its derivative\n",
        "# ReLU(x) = max(0, x)\n",
        "# ReLU'(x) = 1 if x > 0, else 0\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    # YOUR CODE HERE:\
